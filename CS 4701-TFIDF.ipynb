{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import re\n",
    "from cleantext import clean\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Copy of Bot Data.xlsx')\n",
    "\n",
    "#text preprocessing\n",
    "\n",
    "#convert text to lower case \n",
    "df['biography']=df['biography'].str.lower()\n",
    "df['comment']=df['comment'].str.lower()\n",
    "\n",
    "#expand contractions \n",
    "contractions_dict = {\"aren't\": \"are not\", \"don't\": \"do not\", \"Don't\": \"do not\", \"I'm\": \"I am\", \"i'm\": \"I am\", \n",
    "                    \"it's\": \"it is\", \"y'all\": \"you all\", \"Y'all\": \"you all\", \"didn't\": \"did not\", \"won't\": \"will not\",\n",
    "                   \"I'll\": \"I will\", \"i'll\": \"I will\", \"can't\": \"can not\"}\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(text, expand_dict):\n",
    "    def replace(match):\n",
    "        return expand_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "df['biography']=df['biography'].apply(lambda x:expand_contractions(x, contractions_dict ))\n",
    "df['comment']=df['comment'].apply(lambda x:expand_contractions(x, contractions_dict))\n",
    "bios=df['biography'].astype(str)\n",
    "usernames=df['Username']\n",
    "comments=df['comment']\n",
    "#filter out emojis \n",
    "new_bios=[]\n",
    "for b in bios: \n",
    "    temp=clean(b, no_emoji=True)\n",
    "    temp=temp.replace(\"\\n\", \" \")\n",
    "    if temp=='':\n",
    "        new_bios.append('None')\n",
    "    else: \n",
    "        new_bios.append(temp)\n",
    "new_comments=[]\n",
    "for c in comments:\n",
    "    temp=clean(c, no_emoji=True)\n",
    "    temp=temp.replace(\"\\n\", \" \")\n",
    "    if temp=='':\n",
    "        new_comments.append('None')\n",
    "    else: \n",
    "        new_comments.append(temp)\n",
    "df_filtered=df.copy()\n",
    "df_filtered['biography']=new_bios\n",
    "df_filtered['comment']=new_comments\n",
    "df_filtered['combined']=df_filtered['biography'] + ' ' + df_filtered['comment']\n",
    "from sklearn.model_selection import train_test_split\n",
    "#train and test set split \n",
    "train_set, test_set= train_test_split(\n",
    "df_filtered, test_size=.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>000</th>\n",
       "      <th>0381</th>\n",
       "      <th>05</th>\n",
       "      <th>0598</th>\n",
       "      <th>064acf</th>\n",
       "      <th>09</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>youtu</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yt</th>\n",
       "      <th>zainab</th>\n",
       "      <th>zainabzilahi</th>\n",
       "      <th>zeldon</th>\n",
       "      <th>zero</th>\n",
       "      <th>zilahi</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152454</td>\n",
       "      <td>0.156599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 1942 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       000  0381   05      0598  064acf   09         1        10  100  \\\n",
       "0    0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "1    0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "2    0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "3    0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "4    0.0  0.000000   0.0  0.0  0.205384     0.0  0.0  0.152454  0.156599  0.0   \n",
       "..   ...       ...   ...  ...       ...     ...  ...       ...       ...  ...   \n",
       "246  0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "247  0.0  0.152785   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "248  0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "249  0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "250  0.0  0.000000   0.0  0.0  0.000000     0.0  0.0  0.000000  0.000000  0.0   \n",
       "\n",
       "     ...  youtu  youtube   yt  zainab  zainabzilahi  zeldon  zero  zilahi  \\\n",
       "0    ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "1    ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "2    ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "3    ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "4    ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "..   ...    ...      ...  ...     ...           ...     ...   ...     ...   \n",
       "246  ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "247  ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "248  ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "249  ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "250  ...    0.0      0.0  0.0     0.0           0.0     0.0   0.0     0.0   \n",
       "\n",
       "     zooming   zu  \n",
       "0        0.0  0.0  \n",
       "1        0.0  0.0  \n",
       "2        0.0  0.0  \n",
       "3        0.0  0.0  \n",
       "4        0.0  0.0  \n",
       "..       ...  ...  \n",
       "246      0.0  0.0  \n",
       "247      0.0  0.0  \n",
       "248      0.0  0.0  \n",
       "249      0.0  0.0  \n",
       "250      0.0  0.0  \n",
       "\n",
       "[251 rows x 1942 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vector.fit(train_set['combined'])\n",
    "tfidf_array =  tfidf_vector.transform(train_set['combined']).toarray()\n",
    "text_df = pd.DataFrame(data=tfidf_array,columns = tfidf_vector.get_feature_names())\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5',\n",
       " 'app',\n",
       " 'bio',\n",
       " 'click',\n",
       " 'content',\n",
       " 'dm',\n",
       " 'do',\n",
       " 'first',\n",
       " 'i',\n",
       " 'link',\n",
       " 'linkr',\n",
       " 'ly',\n",
       " 'me',\n",
       " 'my',\n",
       " 'netlify',\n",
       " 'people',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'you']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "selector1=RFE(estimator=model, n_features_to_select=20, step=1) #chose arbitrarily to select 20 of the most important words out of 2000 to make it more manageable\n",
    "\n",
    "#split dataset into features and labels\n",
    "Y1=train_set['bot classification (0-not a bot, 1-bot)']\n",
    "X1=text_df\n",
    "\n",
    "#run RFE\n",
    "rfe1 = selector1.fit_transform(X1, Y1)\n",
    "filter1=(selector1.get_support())\n",
    "\n",
    "##\n",
    "#filter columns using data from RFE\n",
    "filter1=list(filter1)\n",
    "current_cols = list(X1.columns)\n",
    "\n",
    "#figure out which columns to keep\n",
    "important_cols=[]\n",
    "for index in range(len(filter1)):\n",
    "    if (filter1[index])==True:\n",
    "        important_cols.append(current_cols[index])\n",
    "important_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_filtered=train_set[['Follower Count', 'Following Count',\n",
    "       'Follower/Following Ratio', 'Number of posts', '# of likes on comment ', 'time of comment after post (minutes)', 'bot classification (0-not a bot, 1-bot)']]\n",
    "df_normalized=train_set_filtered.copy()\n",
    "numerical_cols=['Follower Count', 'Following Count',\n",
    "       'Follower/Following Ratio', 'Number of posts', '# of likes on comment ', 'time of comment after post (minutes)']\n",
    "for c in numerical_cols:\n",
    "    df_normalized[c]= (train_set_filtered[c] - train_set_filtered[c].min()) / (train_set_filtered[c].max() - train_set_filtered[c].min())\n",
    "df_normalized=df_normalized.reset_index(drop=True)\n",
    "\n",
    "#normalize the test set \n",
    "test_normalized=test_set.copy()\n",
    "test_normalized=test_normalized[['Follower Count', 'Following Count',\n",
    "       'Follower/Following Ratio', 'Number of posts', '# of likes on comment ', 'time of comment after post (minutes)', 'bot classification (0-not a bot, 1-bot)']]\n",
    "for c in numerical_cols: \n",
    "    test_normalized[c]= (test_normalized[c] - test_normalized[c].min()) / (test_normalized[c].max() - test_normalized[c].min())  \n",
    "test_normalized=test_normalized.reset_index(drop=True)\n",
    "\n",
    "#convert the text columns to arrays and merge dataframes \n",
    "text_array_test = tfidf_vector.transform(test_set['combined']).toarray()\n",
    "text_df_test = pd.DataFrame(data=text_array_test,columns = tfidf_vector.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "features_filtered=X1[important_cols]\n",
    "#merge with original numerical columns\n",
    "features_filtered=pd.concat([features_filtered, df_normalized], axis=1)\n",
    "X_train=features_filtered.drop(['bot classification (0-not a bot, 1-bot)'], axis=1)\n",
    "Y_train=features_filtered['bot classification (0-not a bot, 1-bot)']\n",
    "\n",
    "#filter test set for important columns \n",
    "text_df_test=text_df_test[important_cols]\n",
    "df_combined_test=pd.concat([text_df_test, test_normalized], axis=1)\n",
    "df_combined_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_combined_test=df_combined_test.dropna()\n",
    "X_test=df_combined_test.drop(['bot classification (0-not a bot, 1-bot)'], axis=1)\n",
    "Y_test=df_combined_test['bot classification (0-not a bot, 1-bot)']\n",
    "\n",
    "#logistic regression \n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "score = classifier.score(X_test, Y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948 (0.059)\n"
     ]
    }
   ],
   "source": [
    "#K-fold cross validation again \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "LR = LogisticRegression(max_iter=1000)\n",
    "cv = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "result = cross_val_score(LR, X_train, Y_train, cv=cv, scoring='accuracy')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(result), std(result)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9561568627450981\n",
      "Best Hyperparameters: {'C': 21.54434690031882, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:    3.0s finished\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter tuning \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "h_model=LogisticRegression(max_iter=1000)\n",
    "grid = dict()\n",
    "grid['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "grid['penalty'] = ['l1', 'l2', 'none']\n",
    "grid['C'] = np.logspace(-4, 4, 10) #range of values to test\n",
    "grid_search = GridSearchCV(h_model, grid, cv = 5, scoring = 'accuracy',n_jobs=-1, verbose=True)\n",
    "final= grid_search.fit(X_train, Y_train)\n",
    "print('Best Score: %s' % final.best_score_)\n",
    "print('Best Hyperparameters: %s' % final.best_params_)\n",
    "final_model = final.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'dm', 'ly', 'content', 'people']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at feature importance\n",
    "importance = final_model.coef_[0]\n",
    "indices = (-importance).argsort()[:5]\n",
    "col_names=X_test.columns.tolist()\n",
    "important_features = [col_names[i] for i in indices]\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
